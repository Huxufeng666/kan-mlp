import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from tqdm import tqdm
# æ•°æ®é¢„å¤„ç†
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),  # éšæœºæ°´å¹³ç¿»è½¬
    transforms.RandomCrop(32, padding=4),  # éšæœºè£å‰ª
    transforms.ToTensor(),  # è½¬æ¢ä¸ºå¼ é‡
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # æ ‡å‡†åŒ–
])

# åŠ è½½ CIFAR-10 æ•°æ®é›†
batch_size = 64
train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)
test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# æ¨¡å‹å®ç°
class CustomCIFAR10Model(nn.Module):
    def __init__(self, n, k, num_classes=100, hidden_dim=64):
        """
        å‚æ•°:
        - n: è¾“å…¥å˜é‡æ•°é‡ï¼ˆå…¬å¼ä¸­çš„ nï¼‰
        - k: åµŒå¥—æ¿€æ´»å‡½æ•°çš„æ•°é‡ï¼ˆå…¬å¼ä¸­çš„ kï¼‰
        - num_classes: CIFAR-10 åˆ†ç±»æ•°ç›®ï¼ˆ10ç±»ï¼‰
        - hidden_dim: ä¸­é—´éšè—å•å…ƒæ•°é‡
        """
        super(CustomCIFAR10Model, self).__init__()
        
        # å·ç§¯å±‚æå–ç‰¹å¾
        self.conv = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # è¾“å…¥é€šé“ 3ï¼Œè¾“å‡ºé€šé“ 32
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),  # ä¸‹é‡‡æ ·
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)
        )
        
        # ç¬¬ä¸€éƒ¨åˆ†ï¼šçº¿æ€§åŠ æƒå’Œ
        self.w = nn.Parameter(torch.randn(n))  # w_i
        self.n = nn.Parameter(torch.randn(n))  # n_i
        
        # ç¬¬äºŒéƒ¨åˆ†ï¼šåµŒå¥—æ¿€æ´»å‡½æ•° Ï†_j å’Œçº¿æ€§å±‚
        self.phi_j = nn.ModuleList([nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        ) for _ in range(k)])  # å®šä¹‰ Ï†_j
        
        # åµŒå¥—çº¿æ€§å˜æ¢
        self.linear = nn.Linear(64 * 8 * 8, hidden_dim)  # å±•å¹³åçš„ç‰¹å¾æ˜ å°„
        
        # è¾“å‡ºåˆ†ç±»å±‚
        self.fc = nn.Linear(hidden_dim, num_classes)  # åˆ†ç±»å±‚
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­å®ç°:
        - x: è¾“å…¥å›¾åƒå¼ é‡, å½¢çŠ¶ä¸º [batch_size, 3, 32, 32]
        """
        # å·ç§¯ç‰¹å¾æå–
        x = self.conv(x)  # è¾“å‡ºå½¢çŠ¶ [batch_size, 64, 8, 8]
        x = x.view(x.size(0), -1)  # å±•å¹³ç‰¹å¾æ˜ å°„ä¸º [batch_size, 64 * 8 * 8]
        
        # ç¬¬ä¸€éƒ¨åˆ†: åŠ æƒå’Œ
        part1 = 0
        for i in range(1, len(self.n)):
            # part1 += self.w[i] * self.n[i] + self.w[i-1] * self.n[i-1]
            part1 = self.w[i] * self.n[i] + self.w[i-1] * self.n[i-1]
        
        # ç¬¬äºŒéƒ¨åˆ†: åµŒå¥—æ¿€æ´»å’Œçº¿æ€§å˜æ¢
        linear_out = self.linear(x)  # âˆ‘_(ğ‘–=0)^ğ‘› çš„çº¿æ€§å˜æ¢
        part2 = 0
        for j, phi in enumerate(self.phi_j):
            # part2 += phi(linear_out)  # é€šè¿‡æ¯ä¸ª Ï†_j çš„åµŒå¥—éçº¿æ€§
            part2 = phi(linear_out)  # é€šè¿‡æ¯ä¸ª Ï†_j çš„åµŒå¥—éçº¿æ€§
        
        # åˆå¹¶ä¸¤éƒ¨åˆ†ç»“æœ
        # Z = part1 + part2.sum(dim=1)
        Z = part1 + part2
        
        # åˆ†ç±»è¾“å‡º
        output = self.fc(Z)
        return output

# æ¨¡å‹å®ä¾‹åŒ–
n = 5  # è¾“å…¥å˜é‡æ•°é‡
k = 3  # åµŒå¥—æ¿€æ´»å‡½æ•°çš„æ•°é‡
model = CustomCIFAR10Model(n=n, k=k)

# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()  # åˆ†ç±»ä»»åŠ¡æŸå¤±
optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)


# æµ‹è¯•æ¨¡å‹å‡½æ•°
def test_model(model, test_loader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        # æ·»åŠ è¿›åº¦æ¡
        with tqdm(test_loader, desc="Testing") as pbar:
            for images, labels in pbar:
                images, labels = images.cuda(), labels.cuda()
                outputs = model(images)
                _, predicted = torch.max(outputs, 1)  # è·å–é¢„æµ‹ç±»åˆ«
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

                # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
                accuracy = 100 * correct / total
                pbar.set_postfix(accuracy=accuracy)

    # æ‰“å°æœ€ç»ˆæ­£ç¡®ç‡
    final_accuracy = 100 * correct / total
    print(f"Test Accuracy: {final_accuracy:.2f}%")
    return final_accuracy


# è®­ç»ƒæ¨¡å‹å‡½æ•°
def train_model(model, train_loader, criterion, optimizer, num_epochs=100):
    model.train()
    for epoch in range(num_epochs):
        total_loss = 0
        correct = 0
        total = 0

        # æ·»åŠ è¿›åº¦æ¡
        with tqdm(train_loader, desc=f"Epoch [{epoch+1}/{num_epochs}]") as pbar:
            for images, labels in pbar:
                images, labels = images.cuda(), labels.cuda()  # ä½¿ç”¨ GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
                
                # å‰å‘ä¼ æ’­
                outputs = model(images)
                loss = criterion(outputs, labels)
                
                # åå‘ä¼ æ’­å’Œä¼˜åŒ–
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()

                # è®¡ç®—æ­£ç¡®ç‡
                _, predicted = torch.max(outputs, 1)  # è·å–æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç±»åˆ«
                correct += (predicted == labels).sum().item()
                total += labels.size(0)

                # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
                accuracy = 100 * correct / total
                pbar.set_postfix(loss=loss.item(), accuracy=accuracy)

        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f},Train Accuracy: {accuracy:.2f}%")
        
        print(f"\nTesting after epoch {epoch+1}:")
        test_model(model, test_loader)  # è°ƒç”¨æµ‹è¯•å‡½æ•°



# å¼€å§‹è®­ç»ƒ
model = model.cuda()  # å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU
train_model(model, train_loader, criterion, optimizer, num_epochs=100)
test_model(model, test_loader)
